defaults: # which model/schedule config file to use
- model: sphnet
- schedule: polynomial 
- _self_

job_id: nabla100k_200k_lr3e-4_bs16*8_sparse0.4_ema # The checkpoint/log file is saved in ckpt_path/job_id
# job_id: debug
ckpt_path: outputs
seed: 123
log_dir: "outputs"
debug: false

wandb: # wandb configuration
  open: False
  wandb_api_key: 3a6f474a0825b58ee4aa3628a434644b1ed279b6
  wandb_project: your_wandb_project
  wandb_group: ""
  wandb_notes: ""

#########
# trainer related config
model_backbone: SPHNet

hami_model:
  name: HamiHead_sphnet
  irreps_edge_embedding: null
  num_layer: 2
  max_radius_cutoff: 30  # hami head cut off, default full connected
  radius_embed_dim: 16
  bottle_hidden_size: 32

use_sparse_tp: true # whether use the sparse tensor product gate
sparsity: 0.4 #Sparsity ratio
num_epochs: 3000000 #Maximum number of epochs
max_steps: 200000 #Maximum number of training steps.
batch_size: 16
inference_batch_size: 1
dataloader_num_workers: 8
lr: 3e-4
multi_para_group: false
weight_decay: 0
enable_hami: true
enable_energy_hami_error: false # whether to calculate the occupiedenergy MAE and C similarity during validation. This is very slow.
enable_hami_orbital_energy: false # set it to true for WALoss
hami_train_loss: 'maemse'
hami_val_loss: 'mae'
ngpus: 8
num_nodes: 1
precision: 32 # the precison for training, can use 16 for mixed precision
gradient_clip_val: 5.0
early_stopping_patience: 300000
test_interval: 10 #Test interval, one test per n epochs (default: 10) # NOT used
save_interval: 1 #Save interval, one save per n epochs (default: 10)
############
# data realted config
# data_name: pubchem  #the target dataset
# basis: "def2-tzvp"  #when predict hamitonian, the basis need to be set
# dataset_path : "/data/pubchem/"  # if the dataset is lmdb format, enter the folder path

# data_name: qh9_stable_iid # four kinds of data splits: qh9_stable_iid, qh9_stable_ood, qh9_dynamic_mol, qh9_dynamic_geo
# basis: "def2-svp"
# dataset_path : "/data/qh9stable/data.mdb"     # if the dataset is mdb format, enter the file path
# # dataset_path : "/data/qh9dynamic/data.mdb"

# data_name: md17
# basis: "def2-svp"
# dataset_path : "/data/md17/ethanol/data.mdb"

data_name: custom_nabla
basis: "def2-svp"
dataset_path : "datasets/nabladft_preprocessed/train_100k.db/data.mdb"
val_path: "datasets/nabladft_preprocessed/test_100k_conformers.db/data.mdb"
test_path: "datasets/nabladft_preprocessed/test_100k_conformers.db/data.mdb"
index_path : "."

# train_size: 1000 # limit train size to N samples
# train_ratio: 0.8 # limit training set ratio; default: 1.0
used_cache: false
ema_decay: 0.99 # 1 means trun off ema and (0,1) is turning on.
unit: 1 # 627.503 if the unit in the dataset is kcal/mol, else set 1; the unit used in this model is hatree
############
# model related config
activation: "silu"
remove_init: true # fock - init if remove_init else fock
remove_atomref_energy: true # when true, energy = energy - eachtype_atom_count*atom_ref
num_sanity_val_steps: 2 # number of validation steps for sanity check
limit_val_batches: 0.1 # float: proportion / int: batches
val_check_interval: 0.5 # null follow pytorch lightning; float for epoch; int for batches
check_val_every_n_epoch: 1 # 同时设置 val_check_interval / check_val_every_n_epoch，以val_check_interval为准
